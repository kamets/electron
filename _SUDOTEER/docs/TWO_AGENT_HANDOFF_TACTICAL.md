# Two-Agent Handoff System - Tactical Implementation

**Date**: 2025-12-23
**Based on**: Anthropic's Agentic Workflow whitepaper (proven in production)
**Status**: üî¥ **CRITICAL UPGRADE**

---

## üéØ **CORE ARCHITECTURE (Confirmed)**

### **Two-Agent Pattern**:

```
[Initializer Agent] ‚Üí Sets up project
        ‚Üì
[Coding Agent Loop] ‚Üí Picks one task ‚Üí Implements ‚Üí Tests ‚Üí Updates artifacts ‚Üí Repeat
        ‚Üì
Completion when: All tests pass
```

**Memory**: `claude-progress.txt` + Git History + `feature_list.json`

---

## üìã **CRITICAL TACTICAL DETAIL: Feature List Schema**

### **Why JSON over Markdown**:
> "Models are less likely to inappropriately change or overwrite JSON files compared to Markdown files"

### **Exact Schema** (from Anthropic):
```json
{
  "features": [
    {
      "category": "functional",
      "description": "New chat button creates a fresh conversation",
      "steps": [
        "Navigate to main interface",
        "Click the 'New Chat' button",
        "Verify a new conversation is created"
      ],
      "passes": false,
      "last_tested": "2025-12-23T10:00:00Z",
      "agent_notes": ""
    }
  ]
}
```

**Key Insight**: Not just a to-do list - it's a **test specification** with verification steps!

---

## ‚ö†Ô∏è **FAILURE MODES (Why We Need This)**

### **1. One-Shotting** üî¥
**Problem**: Agent tries to build entire app in single run
**Result**: Context overflow, incomplete work
**Solution**: Feature list forces incremental progress (one feature at a time)

### **2. Premature Victory** üî¥
**Problem**: Agent looks at half-finished app and declares "done"
**Result**: Broken deliverable
**Solution**: `"passes": false` in feature list - objective test results

### **3. Guesswork** üî¥
**Problem**: Agent wastes time figuring out how to start
**Result**: Repeated environment setup failures
**Solution**: `init.sh` script - reproducible environment every time

---

## üõ†Ô∏è **REVISED ARTIFACT SYSTEM**

### **1. feature_list.json** (Enhanced)

```json
{
  "total_features": 10,
  "completed": 3,
  "in_progress": 1,
  "failed": 1,
  "not_started": 5,
  "features": [
    {
      "id": "feat_001_validation_chain",
      "category": "functional",
      "description": "Validation chain processes all agents sequentially",
      "priority": "critical",
      "steps": [
        "Coder creates BaseValidator class",
        "Tester writes 20 unit tests",
        "Documenter adds JSDoc",
        "Validator runs compliance check",
        "All tests pass (pytest)"
      ],
      "passes": false,
      "tests_passing": 15,
      "tests_total": 20,
      "coverage": 88.4,
      "assigned_to": "coder_01",
      "started_at": "2025-12-23T09:00:00Z",
      "last_tested": "2025-12-23T10:15:00Z",
      "agent_notes": "3 edge case tests still failing",
      "test_command": "pytest tests/test_validator.py",
      "verification_method": "automated"
    },
    {
      "id": "feat_002_greenhouse_control",
      "category": "functional",
      "description": "UI can control greenhouse pump via websocket",
      "priority": "high",
      "steps": [
        "Click pump button in UI",
        "Verify command sent to Python backend",
        "Verify pump_active = True in telemetry",
        "Verify water pressure rises to 40 PSI"
      ],
      "passes": true,
      "tests_passing": 5,
      "tests_total": 5,
      "coverage": 95.2,
      "test_command": "pytest tests/test_greenhouse_control.py",
      "verification_method": "e2e",
      "verification_tool": "playwright"
    }
  ]
}
```

**Key Additions**:
- ‚úÖ `steps`: Test steps (like Puppeteer scenarios)
- ‚úÖ `passes`: Objective pass/fail (not guesswork)
- ‚úÖ `test_command`: How to verify
- ‚úÖ `verification_method`: `automated` | `e2e` | `manual`
- ‚úÖ `verification_tool`: `pytest` | `playwright` | `puppeteer`

---

### **2. init.sh** (Environment Bootstrap)

```bash
#!/bin/bash
# _SUDOTEER Environment Initialization
# Auto-generated by InitializerAgent

set -e  # Exit on error

echo "üöÄ Initializing _SUDOTEER workspace..."

# 1. Python Virtual Environment
if [ ! -d ".venv" ]; then
    echo "Creating Python virtual environment..."
    python -m venv .venv
fi

source .venv/Scripts/activate  # Windows
# source .venv/bin/activate    # Linux/Mac

# 2. Install Dependencies
echo "Installing Python dependencies..."
pip install -q -r requirements.txt

# 3. Install Frontend Dependencies
if [ -d "frontend" ]; then
    echo "Installing Node dependencies..."
    cd frontend
    npm install --silent
    cd ..
fi

# 4. Verify Environment
echo "Verifying installations..."
python -c "import dspy; print('‚úÖ DSPy installed')"
python -c "import supabase; print('‚úÖ Supabase client installed')"

# 5. Start Backend (optional)
# python backend/agency.py &
# BACKEND_PID=$!

# 6. Start Frontend (optional)
# cd frontend && npm start &
# FRONTEND_PID=$!

echo "‚úÖ Environment ready!"
echo "To activate: source .venv/Scripts/activate"
echo "To run tests: pytest tests/"
echo "To start agency: python backend/agency.py"
```

**Purpose**: Zero guesswork - environment is identical every run

---

### **3. claude-progress.txt** (Enhanced Handoff)

```
=== SESSION 2025-12-23 10:15 AM ===

CURRENT FEATURE: feat_001_validation_chain
STATUS: In Progress (15/20 tests passing)

LAST AGENT: tester_01
WHAT WAS DONE:
- Created 20 unit tests for BaseValidator
- 15 tests passing
- 5 edge case tests failing (see test_validator.py:L127-L145)

NEXT AGENT: coder_01
WHAT TO DO:
- Fix failing edge case tests
- Specifically: handle null input, empty string, malformed JSON
- Do NOT refactor working code
- Run 'pytest tests/test_validator.py' to verify

CONTEXT:
Working on validation chain for forensic audit system.
Current coverage: 88.4%
Target: 95%+

STATE FILES UPDATED:
- feature_list.json: Updated feat_001 with test results
- agent_state.json: Iteration 42

CONTINUE FROM: tests/test_validator.py:L127

--- DO NOT EDIT ABOVE THIS LINE ---
(Agent will append notes below)
```

**Prevents Premature Victory**: Next agent knows **exactly** what's broken

---

## üî¨ **VERIFICATION STRATEGY**

### **Levels of Testing**:

#### **1. Unit Tests (pytest)**
```json
{
  "verification_method": "automated",
  "verification_tool": "pytest",
  "test_command": "pytest tests/test_validator.py -v"
}
```

#### **2. E2E Tests (Playwright/Puppeteer)**
```json
{
  "verification_method": "e2e",
  "verification_tool": "playwright",
  "test_command": "pytest tests/e2e/test_greenhouse_ui.py",
  "steps": [
    "Launch Electron app",
    "Click greenhouse pump button",
    "Verify button text changes to 'STOP PUMP'",
    "Verify telemetry shows pump_active = true",
    "Verify water pressure rises"
  ]
}
```

#### **3. Manual Verification (HITL)**
```json
{
  "verification_method": "manual",
  "verification_tool": "human",
  "steps": [
    "Launch app",
    "Human tester verifies UI looks correct",
    "Human approves before deployment"
  ]
}
```

---

## ü§ñ **TWO-AGENT IMPLEMENTATION**

### **Agent 1: InitializerAgent**

**Purpose**: Bootstrap project with zero ambiguity

**Outputs**:
1. `init.sh` - Environment setup script
2. `feature_list.json` - Complete test specification
3. `claude-progress.txt` - Initial handoff note
4. `agent_state.json` - Session initialization

**Implementation**:
```python
class InitializerAgent(SudoAgent):
    async def forward(self, project_spec: str) -> Dict:
        """
        Bootstrap project from specification.

        Creates:
        1. init.sh with all dependencies
        2. feature_list.json with test specifications
        3. Initial handoff note
        """
        # 1. Parse project spec
        features = self._extract_features(project_spec)

        # 2. Generate init.sh
        init_script = self._generate_init_script(features)
        Path("init.sh").write_text(init_script)

        # 3. Create feature_list.json
        feature_list = {
            "total_features": len(features),
            "completed": 0,
            "features": [
                {
                    "id": f"feat_{i:03d}",
                    "category": feat.category,
                    "description": feat.description,
                    "steps": feat.steps,
                    "passes": False,
                    "test_command": feat.test_command
                }
                for i, feat in enumerate(features)
            ]
        }
        Path("feature_list.json").write_text(json.dumps(feature_list, indent=2))

        # 4. Create initial handoff
        handoff = f"""
=== INITIALIZATION COMPLETE ===

PROJECT: {project_spec}
TOTAL FEATURES: {len(features)}

NEXT AGENT: coding_agent
WHAT TO DO:
1. Run './init.sh' to set up environment
2. Pick first feature from feature_list.json where passes=false
3. Implement the feature
4. Run the test_command
5. Update feature_list.json with results
6. Append notes to this file

START WITH: Feature feat_001

--- CODING AGENT STARTS BELOW ---
"""
        Path("claude-progress.txt").write_text(handoff)

        return {"status": "initialized", "features": len(features)}
```

---

### **Agent 2: CodingAgent (Loop)**

**Purpose**: Incremental implementation with verification

**Loop**:
```python
while True:
    # 1. Read state
    progress = Path("claude-progress.txt").read_text()
    features = json.loads(Path("feature_list.json").read_text())

    # 2. Verify environment
    subprocess.run(["bash", "init.sh"], check=True)

    # 3. Pick next feature
    next_feature = next((f for f in features["features"] if not f["passes"]), None)

    if not next_feature:
        print("‚úÖ All features complete!")
        break

    # 4. Implement feature
    code = await self.implement_feature(next_feature)

    # 5. Run tests
    result = subprocess.run(
        next_feature["test_command"],
        shell=True,
        capture_output=True
    )

    # 6. Update feature list
    next_feature["passes"] = (result.returncode == 0)
    next_feature["last_tested"] = datetime.now().isoformat()

    if not next_feature["passes"]:
        next_feature["agent_notes"] = result.stderr.decode()

    Path("feature_list.json").write_text(json.dumps(features, indent=2))

    # 7. Update progress
    with open("claude-progress.txt", "a") as f:
        f.write(f"\n\n[{datetime.now()}] Completed {next_feature['id']}: {'PASS' if next_feature['passes'] else 'FAIL'}\n")

    # 8. Git commit
    subprocess.run(["git", "add", "."])
    subprocess.run(["git", "commit", "-m", f"feat: {next_feature['description']}"])
```

---

## üìä **ANTI-PATTERNS (What NOT to Do)**

### **‚ùå One-Shotting**:
```python
# BAD: Try to do everything at once
async def build_entire_app():
    code = await llm.generate("Build complete forensic audit app")
    # Result: Context overflow, incomplete work
```

### **‚úÖ Incremental**:
```python
# GOOD: One feature at a time
for feature in features:
    code = await llm.generate(f"Implement: {feature['description']}")
    test_result = run_tests(feature['test_command'])
    feature['passes'] = test_result.success
```

---

### **‚ùå Premature Victory**:
```python
# BAD: Agent guesses if it's done
if looks_complete(codebase):
    return "Done!"  # Dangerous guess
```

### **‚úÖ Objective Tests**:
```python
# GOOD: Objective pass/fail
all_pass = all(f['passes'] for f in features)
if all_pass:
    return "Verified complete"
```

---

### **‚ùå Guesswork**:
```python
# BAD: Agent tries to figure out environment
await llm.generate("How do I start this app?")
# Result: Wasted iterations
```

### **‚úÖ init.sh**:
```bash
# GOOD: Scripted, reproducible
./init.sh  # Always works the same way
```

---

## üéØ **IMPLEMENTATION CHECKLIST**

### **Phase 1: Update Artifacts** ‚úÖ (Partially Done)
- [x] Create ArtifactManager
- [x] Create ContextEngine
- [ ] **Enhance feature_list.json schema** ‚Üê THIS IS CRITICAL
- [ ] Create init.sh template generator
- [ ] Enhance claude-progress.txt format

### **Phase 2: Create InitializerAgent** (3-4 hours)
- [ ] Implement InitializerAgent class
- [ ] Add init.sh generation
- [ ] Add feature_list.json generation
- [ ] Test project bootstrap

### **Phase 3: Create CodingAgent Loop** (4-6 hours)
- [ ] Implement CodingAgent class
- [ ] Add iterative loop logic
- [ ] Add test execution integration
- [ ] Add feature_list.json updates

### **Phase 4: E2E Testing Integration** (4-6 hours)
- [ ] Install Playwright
- [ ] Create E2E test templates
- [ ] Add UI verification tests
- [ ] Integrate with feature_list.json

---

## üöÄ **EXPECTED RESULT**

### **Before**:
- Agents work in one-shot mode
- No verification
- Manual test tracking
- Guesswork on environment

### **After**:
- ‚úÖ InitializerAgent bootstraps project
- ‚úÖ CodingAgent iterates feature-by-feature
- ‚úÖ Objective pass/fail via tests
- ‚úÖ Zero guesswork (init.sh)
- ‚úÖ Complete audit trail (Git + artifacts)

**Quality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Production-Proven Pattern**

---

**Status**: üìã **READY TO IMPLEMENT**
**Based on**: Anthropic's proven production system
**Confidence**: üü¢ **VERY HIGH**

*_SUDOTEER Two-Agent Handoff - Tactical Implementation*
